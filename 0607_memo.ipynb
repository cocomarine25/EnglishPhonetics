{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cocomarine25/EnglishPhonetics/blob/main/0607_memo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "0zTXTTmP-Dxb"
      },
      "source": [
        "# 시험 전 마지막 수업"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기본 목차\n",
        "\n",
        "리니어 엘지브라\n",
        "\n",
        "뉴런 네트워크\n",
        "\n",
        "기본적인 llm(메모리가 가장 문제가 된다.) - 8빌리언 파라미터(행렬의 내부 숫자)라면 메모리가 얼마나 필요한가?)\n",
        "\n",
        "프리시즌\n",
        "\n",
        "허깅페이스에서 모델 불러와서 해보는거(llm이 프리트레이닝으로 훈련하는 것과 파인 튜닝으로 훈련하는 것이 다르다.) 그리고 쓰는 것\n",
        "\n",
        "leg가 무엇인지"
      ],
      "metadata": {
        "id": "-YRmQy4sS_Oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 교수님 피피티\n",
        "\n",
        "벡터는 입력벡터, 행렬, 출력벡터\n",
        "행렬은 여라개일 수 있다.\n",
        "\n",
        "챗gpt는 175B파라미터(1750억)\n",
        "\n",
        "AI행렬은 펑션\n",
        "\n",
        "LM은 랭기지 모델\n",
        "\n",
        "STT는 음성인식 스픽 투 텍스트\n",
        "\n",
        "이미지 레코그나이즈\n",
        "\n",
        "이미지 그레어쩌구(생성)\n",
        "\n",
        "LLM이 좋은건 레이턴시가 작고, ... 등\n",
        "\n",
        "여기까지는 기본이라 다들 알고있어야함"
      ],
      "metadata": {
        "id": "DoAjV0AkTgTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Net basics.pdf\n",
        "\n",
        "입력이 하나 출력이 하나인 경우에 가장 작다.\n",
        "\n",
        "예를들어 커피를 얼마나 마셨는지에 따라 사람의 수면 시간을 예상할 수 있다.(입력 : 커피 몇잔?, 출력 : 수면 시간)\n",
        "\n",
        "y=ax+b에서 a와b값을 구하는 것이 AI다.\n",
        "\n",
        "ANN DNN그림에서 X는 입력, 화살표는 행렬, 그 다음hidden_1은 중간벡터 --> 마지막 벡터가 출력\n",
        "\n",
        "경제학이나 경영학의 회귀분석이 AI에 사용된다.\n",
        "\n",
        "다차원의 벡터가 다차원의 벡터로 가는 것을 ANN DNN이라고 한다.\n",
        "\n",
        "시험문제에 그림을 띄울 수도 있다.(설명한 부분만)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lJ_nVZv1UylG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN이 우리가 하는 것이다.(시간적인 부분)\n",
        "\n",
        "RNN은 해석을 잘 해야하는데\n",
        "\n",
        "첫번째 RNN ppt는 동그라미 하나지만 그 안에 여러개의 벡터가 있다고 생각하면 된다. 말 그대로 위의 ANN DNN그림에서 Wh0, Whh, Wih가 포함되어 있는 것이다.(이전의 중간벡터와 현재의 중간벡터를 연결해주는 화살 벡터가 있다.)\n",
        "\n",
        "T1을 위한 행렬 T2를 위한 행렬이 필요 없이 벡터 하나로 바뀐 것이다.\n",
        "\n",
        "그러니까 RNN 오른쪽 그림을 보면 순서가 아래 위, 다시 아래 위, 다시 아래 위 이렇게 된다.\n",
        "\n",
        "데이터를 이용해서 화살표를 업데이트하는 것을 훈련이라고 하고,\n",
        "\n",
        "실제로 '나는'을 입력하면 '오늘'이 나와야하는데 다른 것이 나오면 그것이 loss이다. 이 loss가 Wih, Whh, Who로 들어간다. 이것이 훈련이다.\n",
        "\n",
        "\n",
        "\n",
        "예를 들어 2020년 주식값을 x1에 넣으면, 그 값이 y1로 나오게 된다. 이렇게 하더라도 한국어에서 영어로 번역을 할 때 입력 길이와 출력 길이와 다르거나 1대1 매치가 되지 않는다면 문제가 생기기도 한다."
      ],
      "metadata": {
        "id": "_mNN7c-gYltD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그래서 생긴 것이 시퀀스 투 시퀀스다.\n",
        "\n",
        "seq2seq\n",
        "\n",
        "여기에서 트렌스포머라는 개념이 나온다.\n",
        "\n",
        "트랜스포머는 동시에 하는 것이다. x1, x2, x3의 값을 동시에 구한다.\n",
        "\n",
        "그 다음 구해진 값을 모두 곱한다?(sum이긴 함)\n",
        "\n",
        "다시 말해 go의 값을 구하려면 나는, 집에, 간다 중에서 가장 영향력을 많이 미친 간다 부분이 go로 오게 되는 것이다.\n",
        "\n",
        "앞부분을 인코드 대충 sos쪽부터 뒷부분을 디코드라고 하는데 챗지피티는 디코드를 사용한다.\n",
        "\n",
        "훈련하는 방법은 데이터의 정제도 필요 없이 아무 문장이나 집에넣으면 되는 것이다.\n",
        "\n",
        "순서대로 추측이 되게 한다. sos(문장 처음)이 I라면 그 다음은 go가 오고 그 다음은 home이 온다.\n",
        "\n",
        "이것이 프리트레이닝이다. llm의 프리트레이닝은 문장을 계속 넣으면서 그 다음 단어를 예측하면된다. 예측하기 이전에 나왔던 모든 것을 파악하고 있으면서 해야한다. i don't like to 다음에 뭐가 나온다는 것을 알아야지, 그냥 to 다음에 뭐가 나온다는 것만 알면 안된다.\n",
        "\n",
        "80억개 이상은 있어야 틀리지 않는다. 근데 말이 틀리지 않는 거랑 정보를 제대로 주는 것은 다르다.\n",
        "\n",
        "트레이닝은 pretraing(문장만 있으면 됨)이 있고 fine training(문장이 QA의 형태로 세팅되어야 함)이 있다.\n",
        "\n",
        "문장을 QA 형식으로 pretraing처럼 계속 돌리면 그것이 파인 트레이닝인 것이다."
      ],
      "metadata": {
        "id": "JFQ37GMuYtWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# basicterms.pdf\n",
        "\n",
        "- CPU는 뉴코어(계산할 수 있는 주체가 6개 있는 것)\n",
        "\n",
        "스티브잡스같은 천재\n",
        "\n",
        "- 지포스 RTX에서 24GB부분이 중요함(스트림 프로세서 16384가 머리수다)\n",
        "\n",
        "곱하기 더하기만 하는 초등학생\n",
        "\n",
        "-Llama3의 개수는 params의 크기다. 8B는 참치같은 것 70B는 고래같은것. 그래서 요리하기 위해서 큰 도마가 필요하다.\n",
        "\n",
        "이걸 계산하는 방법이 다음 장에 MAX, min이다.\n",
        "\n",
        "32비트는 01하는 것을 32개를 쓰는 것\n",
        "\n",
        "70B에서 B는 0을 9개 더 추가해야 됨.\n",
        "\n",
        "이거 계산할줄 알아야됨.\n",
        "\n",
        "GPU memory for training? 훈련할 때는 3~5배 이상의 메모리가 필요하다.\n",
        "\n",
        "memory for inference? 그냥 사용할 때는 그대로 메모리가 필요하다.\n",
        "\n",
        "그 아래 float32 그림은 시험에 안나옴.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_oVNXeR1bye-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear algebra\n",
        "# LA_2024Spring.pdf\n",
        "\n",
        "벡터는 한줄짜리 숫자다.\n",
        "\n",
        "표현은 기하적으로 가능하다.(그래프 그릴 수 있다는 뜻)\n",
        "\n",
        "기하적으로 할 떄는 칼럼 벡타, 로우 벡타 상관 없다.\n",
        "\n",
        "multoplication : 곱하면 두배가 된다.\n",
        "\n",
        "addition : 더하면 합 만큼 된다.\n",
        "\n",
        "\n",
        "\n",
        "[6, -2, 4], [-1, 0, 2] : 벡터 한 점과 다른 벡터 한 점과 원점이 이루는 것은 평면에 있다. - 맞는 말이다.(3차원이든 10차원이든 맞다.)\n",
        "\n",
        "3차원 벡터 하나랑 원점이랑 이루는 것은 1차원이다. 직선이니까\n",
        "\n",
        "3차원 벡터 3개랑 원점이랑 이루는 것은 3차원이다.\n",
        "\n",
        "3차원 벡터 2개랑 원점이 1차원을 이루려면 3차원 벡터 2개가 일직선에 위치하면 되는 것이다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0V12p5HhdJMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "inner product - 이번 학기에 가장 중요함\n",
        "\n",
        "[2, 3]은 1 by 2다.\n",
        "a와 b는 곱해지지 않는다.\n",
        "1x2랑 1x2는 곱해지지 않는다. 1xn, nx1이렇게 같은 부분이 만나야 곱할 수 있다.\n",
        "\n",
        "그래서 이렇게 곱할 수 있도록 만드는 과정이 algebraic이다.\n",
        "\n",
        "그래서 axb 이렇게 쓰면 틀린 것이다. a와 b는 둘다 1x2이기 때문이다. 그래서 a.b 이렇게 표현해야 한다.\n",
        "\n",
        "두 벡터를 이너프로덕트 한다는 말은 기하적으로 두 벡터가 있고, 원점이 있는데 한 벡터를 다른 벡터쪽으로 수직으로 내리는 것이다.\n",
        "\n",
        "그게 |a||b|cos()이다. 그리고 이게 geometric이다.\n",
        "\n",
        "기본적으로 벡타는 AI에 들어가는 입출력이고, 인포메이션이다.\n",
        "\n",
        "두 벡터간의 유사도는 각도만 사용한다.(cosine())\n",
        "\n",
        "유사도는 -1~1까지고 0이 가장 유사하지 않은 것. -1은 반비례.\n",
        "\n",
        "a와b의 거리를 유클리디언 디스턴스라고 한다.\n",
        "\n",
        "10차원에서도 차이를 제곱해서 더하고 루트를 씌우면 된다.\n",
        "\n",
        "- correlation은 한 선을 기준으로 선에 많은 점이 붙어있으면 값이 높다고 한다. science가 1에 더 가까운 것\n",
        "\n",
        "- 가장 중요한 것은 correlation = cosine\n",
        "\n",
        "과학 점수와 수학 점수를 모아서 점을 찍을 수 있다.\n",
        "\n",
        "통계 프로그램에 넣지 않고 하나하나씩 해서 R값을 구할 수 있다.\n",
        "\n",
        "inner product signal vectors : 같은 벡터의 inner product는 값이 매우 클 것이다.\n",
        "\n",
        "a와 b는 9차원이다. a.b는? 1이다. a의 모든 값을 각각 제곱하고, b의 모든 값을 각각 제곱하고, 두 절댓값을 곱하고, cos값을 곱함.\n",
        "\n",
        "두 곡선이 90도 만큼 차이가 나면 inner product는 0값이 된다."
      ],
      "metadata": {
        "id": "Cqf1nO_KeUXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear algebra II\n",
        "# LA_2024SpringII.pdf\n",
        "\n",
        "**Ax = b : 트랜스포메이션 메트릭스라고 함**\n",
        "\n",
        "대문자가 행렬, 소문자가 벡터\n",
        "\n",
        "x가 입력, b가 출력\n",
        "\n",
        "여기에서 기하가 정말 중요하다.\n",
        "\n",
        "이 부분은 저번시간에 내가 메모 열심히 했음!\n",
        "\n",
        "대충 벡터의 방향을 바꾸는 것이 transformation이 하는 일이다.\n",
        "\n",
        "이걸 사용해서 사진 상하좌우 바꾸기를 하는 것이다. 트랜스포메이션 행렬을 곱하면 됨.\n",
        "\n",
        "일직선상으로 바뀐 트렌스포메이션은 어떠한 값을 곱하더라도 1차원을 벗어날 수 없다.\n",
        "\n",
        "**A^-1b=x : 역행렬**\n",
        "\n",
        "트랜스포메이션을 거꾸로 하는 것\n",
        "\n",
        "일직선상이라면 역행렬이 존재하지 않는다.\n",
        "\n",
        "determinent는 면적이다.\n",
        "\n",
        "|1 2|\n",
        "\n",
        "|1 2|\n",
        "\n",
        "이건 0임. (아래 1)x(아래 2) - (위에 2)x(아래 1)하면 0이기 떄문이다.\n",
        "\n",
        "면적이 0이면 그것은 일직선 상인거임. ex)1111\n",
        "\n"
      ],
      "metadata": {
        "id": "YiQrnp20ix0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "마지막으로 eigenvector가 중요하다 했다고 했음.\n",
        "\n",
        "정사각형을 가진 행렬이 있을 때 가능하다.\n",
        "\n",
        "Av=b랑 Ax=b랑 같은거다.\n",
        "\n",
        "아이겐 벡터는 2x2에서는 2개가 나오고 3x3에서는 3개가 나온다.\n",
        "\n",
        "transform한 결과의 벡터가 원점과 일직선이 되는 벡터가 eigenvector이다.\n",
        "\n",
        "아이겐 벡터 = 고유 벡터(그 방향 자체를 의미함)\n",
        "\n",
        "아이겐 벡터의 직각도 아이겐 벡터임(즉, 2개가 존재함)\n",
        "\n",
        "대표 아이겐 벡터는 원점으로 부터 기준원을 그리고 그 지점을 지나는 점을 아이겐 벡터라고 하기도 한다. 수치상으로는 무한대의 개수가 존재하긴 한다.\n",
        "\n",
        "모든 가능한 v에서 몇개의 v는 아이겐 벡터로 간다.\n",
        "\n",
        "Av = ㅅv\n",
        "\n",
        "v : eigenvectors\n",
        "\n",
        "ㅅ : eigenvalues\n",
        "\n",
        "아이겐 벡터는 방향, 아이겐 벨류는 값(숫자)\n",
        "\n",
        "링크 사이트에 들어가면 직접 해볼 수 있음.\n",
        "\n",
        "3차원에서도 벡터가 3개 있을 뿐이지 아이겐 벡터는 존재한다. 근데 서로 수직인 3개의 아이겐 벡터가 나올 것이다.\n",
        "\n",
        "아이겐 벡터가 수직으로 표현이 된다는 것은 서로 상관이 없는 것(영어 능력과 언어 능력을 한번에 같이)\n"
      ],
      "metadata": {
        "id": "DUKa00JFmh1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 리뷰를 하지 않았다고 안나오는 것은 아니다."
      ],
      "metadata": {
        "id": "ZGZt0A5Vpx74"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}